{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Main_LDA_codefile.ipynb",
      "authorship_tag": "ABX9TyPi7xOjK/lymR7ngJTecDRp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayanksethi31/Text_Annotating_System/blob/main/Main_LDA_codefile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9ao4ylbA5R_L"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis\n",
        "import nltk\n",
        "import tarfile\n",
        "\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#Gensim components\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "gHynJE3L6IUa",
        "outputId": "e4a1bec1-7f17-4241-d85c-d855dbeaec3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting tools for LDA\n",
        "#!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "import os\n",
        "import string"
      ],
      "metadata": {
        "id": "iZiM7aeH5oQT",
        "outputId": "beca50e2-b43a-498c-c85b-85a716b36810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Process, freeze_support\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "1NN-OUxv53i8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Method to pre-process the documents, i.e. convert to lowercase, remove punctuations, stopwords, perform stemming and lematization\n",
        "def pre_processing(article):\n",
        "    ###Initializing the stopwords corpus\n",
        "    stop_words = stopwords.words('english')\n",
        "    ###Initializing the stemmer\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    \n",
        "    pre_processed_doc = []\n",
        "    ###Gensim pre_process lowercases the words and then tokenizes the words\n",
        "    for token in gensim.utils.simple_preprocess(article):\n",
        "        if token not in stop_words:\n",
        "            stemmed_lematized_token = stemmer.stem(WordNetLemmatizer().lemmatize(token,pos='v'))\n",
        "            pre_processed_doc.append(stemmed_lematized_token)\n",
        "    \n",
        "    ###To check the pre_processed result set this condition to True\n",
        "    check_preprocess_result=False\n",
        "    if check_preprocess_result:\n",
        "        words = []\n",
        "        for word in article:\n",
        "            words.append(word)\n",
        "        print(words)\n",
        "        print('\\n\\n tokenized and lemmatized document: ')\n",
        "        print(pre_processed_doc)\n",
        "    return pre_processed_doc"
      ],
      "metadata": {
        "id": "gH15UjJU5-fF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "FnRQ6hM86AnB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "editorial_sents = brown.sents(categories=['editorial'])\n",
        "editorials=[]\n",
        "for i in range(len(editorial_sents)):\n",
        "    editorials.append(\" \".join(editorial_sents[i]))"
      ],
      "metadata": {
        "id": "JnkkpDVx6EO4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_documents = []\n",
        "for i in range(len(editorials)):\n",
        "    preprocessed_documents.append(pre_processing(editorials[i]))\n",
        "data_compiled['Pre_processed']=preprocessed_documents\n",
        "dictionary = gensim.corpora.Dictionary(preprocessed_documents)"
      ],
      "metadata": {
        "id": "3p04vHgu6Low"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(no_below=20,no_above=0.5,keep_n=100000)"
      ],
      "metadata": {
        "id": "rYTG2UlB6cFc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words_corpus = [dictionary.doc2bow(document) for document in preprocessed_documents]\n",
        "print(\"Length of bag of words corpus:{} must equal the total number of documents\".format(len(bag_of_words_corpus)))\n",
        "print(\"The first pre processed document\")\n",
        "print(preprocessed_documents[0])\n",
        "print(\"Bag of words for the first document\")\n",
        "print(bag_of_words_corpus[0])\n",
        "print(\"Length of bag of words for first document = {}\".format(len(bag_of_words_corpus[0])))\n",
        "print(\"Length of bag of words for second document = {}\".format(len(bag_of_words_corpus[1])))"
      ],
      "metadata": {
        "id": "l1waSoD56fC8",
        "outputId": "e182bb7b-6925-45f9-a842-d98751757de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of bag of words corpus:2997 must equal the total number of documents\n",
            "The first pre processed document\n",
            "['assembl', 'session', 'bring', 'much', 'good']\n",
            "Bag of words for the first document\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
            "Length of bag of words for first document = 4\n",
            "Length of bag of words for second document = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_first_doc = bag_of_words_corpus[0]\n",
        "for i in range(len(bow_first_doc)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_first_doc[i][0],dictionary[bow_first_doc[i][0]],bow_first_doc[i][1]))\n"
      ],
      "metadata": {
        "id": "3Hu8f3FW6hXB",
        "outputId": "8419ab55-08ea-400b-8aa9-c0bb234868a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 0 (\"assembl\") appears 1 time.\n",
            "Word 1 (\"bring\") appears 1 time.\n",
            "Word 2 (\"good\") appears 1 time.\n",
            "Word 3 (\"much\") appears 1 time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = models.TfidfModel(bag_of_words_corpus)##fitting thr tf idf model on the bag of words corpus\n",
        "corpus_tf_idf = tfidf[bag_of_words_corpus]"
      ],
      "metadata": {
        "id": "wrVE_bPe6j1t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc_tf_idf_vector in corpus_tf_idf:\n",
        "    pprint(doc_tf_idf_vector)\n",
        "    break"
      ],
      "metadata": {
        "id": "SuDj6cJ_6lvr",
        "outputId": "4511a8b9-0a4e-4e58-b234-3e9b321a5296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.5607874010024522),\n",
            " (1, 0.5403780026749527),\n",
            " (2, 0.44160179334881866),\n",
            " (3, 0.44552997790614596)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_using_bag_of_words=True\n",
        "if compute_using_bag_of_words:\n",
        "        ###Running LDA using bag of words\n",
        "        #lda_final_model = gensim.models.ldamodel.LdaModel(bag_of_words_corpus, num_topics=10, id2word=dictionary, passes=50, chunksize=250,update_every=0,alpha='auto',iterations=50,minimum_probability=0.2)\n",
        "    lda_final_model = gensim.models.LdaMulticore(bag_of_words_corpus, num_topics=6, id2word=dictionary, passes=50, chunksize=250,iterations=50,minimum_probability=0.2,workers=4)\n",
        "else:\n",
        "        ###Running LDA using TF-IDF\n",
        "    lda_final_model = gensim.models.LdaMulticore(corpus_tf_idf, num_topics=10, id2word=dictionary, passes=50, chunksize=250,iterations=50,minimum_probability=0.2,workers=4)    \n",
        "    ###Save and load LDA model to save time\n",
        "    #lda_final_model.save('lda.model')\n",
        "    #lda_final_model = models.LdaMulticore.load('lda.model')\n",
        "print(\"The topics and the top words in each topic with weights\")\n",
        "ten_topics=lda_final_model.print_topics(num_words=10)\n",
        "for topic in ten_topics:\n",
        "    print(topic)\n",
        "    \n",
        "    ##words occuring in each topic with the weights to the words\n",
        "for idx, topic in lda_final_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "    \n",
        "    ###Performance evaluation, i.e. to check which topic a document belongs to laong with the score/probability\n",
        "print(\"The first pre-processed document\")\n",
        "print(preprocessed_documents[0])\n",
        "    \n",
        "print(\"Checking the score as in to which topic this document belongs to\")\n",
        "for index, score in sorted(lda_final_model[bag_of_words_corpus[0]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_final_model.print_topic(index, 10)))\n",
        "    "
      ],
      "metadata": {
        "id": "nw2pdHwm6ogW",
        "outputId": "ae9fdd09-ec33-413d-f030-8cf3263f9c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The topics and the top words in each topic with weights\n",
            "(0, '0.067*\"state\" + 0.042*\"nation\" + 0.040*\"unit\" + 0.033*\"citi\" + 0.028*\"work\" + 0.026*\"need\" + 0.023*\"new\" + 0.022*\"school\" + 0.020*\"right\" + 0.020*\"industri\"')\n",
            "(1, '0.041*\"war\" + 0.039*\"like\" + 0.032*\"world\" + 0.032*\"mr\" + 0.032*\"time\" + 0.031*\"peac\" + 0.031*\"american\" + 0.028*\"know\" + 0.025*\"west\" + 0.025*\"editor\"')\n",
            "(2, '0.065*\"make\" + 0.057*\"would\" + 0.041*\"year\" + 0.028*\"seem\" + 0.025*\"peopl\" + 0.024*\"help\" + 0.023*\"call\" + 0.023*\"old\" + 0.022*\"use\" + 0.021*\"fact\"')\n",
            "(3, '0.058*\"say\" + 0.044*\"public\" + 0.040*\"mani\" + 0.036*\"go\" + 0.031*\"give\" + 0.029*\"even\" + 0.024*\"question\" + 0.020*\"servic\" + 0.019*\"way\" + 0.019*\"get\"')\n",
            "(4, '0.040*\"new\" + 0.039*\"come\" + 0.036*\"us\" + 0.028*\"leav\" + 0.026*\"test\" + 0.025*\"mr\" + 0.025*\"take\" + 0.022*\"presid\" + 0.021*\"see\" + 0.021*\"must\"')\n",
            "(5, '0.099*\"one\" + 0.029*\"day\" + 0.028*\"man\" + 0.027*\"year\" + 0.025*\"two\" + 0.022*\"general\" + 0.017*\"back\" + 0.017*\"put\" + 0.017*\"church\" + 0.017*\"forc\"')\n",
            "Topic: 0 \n",
            "Words: 0.067*\"state\" + 0.042*\"nation\" + 0.040*\"unit\" + 0.033*\"citi\" + 0.028*\"work\" + 0.026*\"need\" + 0.023*\"new\" + 0.022*\"school\" + 0.020*\"right\" + 0.020*\"industri\"\n",
            "Topic: 1 \n",
            "Words: 0.041*\"war\" + 0.039*\"like\" + 0.032*\"world\" + 0.032*\"mr\" + 0.032*\"time\" + 0.031*\"peac\" + 0.031*\"american\" + 0.028*\"know\" + 0.025*\"west\" + 0.025*\"editor\"\n",
            "Topic: 2 \n",
            "Words: 0.065*\"make\" + 0.057*\"would\" + 0.041*\"year\" + 0.028*\"seem\" + 0.025*\"peopl\" + 0.024*\"help\" + 0.023*\"call\" + 0.023*\"old\" + 0.022*\"use\" + 0.021*\"fact\"\n",
            "Topic: 3 \n",
            "Words: 0.058*\"say\" + 0.044*\"public\" + 0.040*\"mani\" + 0.036*\"go\" + 0.031*\"give\" + 0.029*\"even\" + 0.024*\"question\" + 0.020*\"servic\" + 0.019*\"way\" + 0.019*\"get\"\n",
            "Topic: 4 \n",
            "Words: 0.040*\"new\" + 0.039*\"come\" + 0.036*\"us\" + 0.028*\"leav\" + 0.026*\"test\" + 0.025*\"mr\" + 0.025*\"take\" + 0.022*\"presid\" + 0.021*\"see\" + 0.021*\"must\"\n",
            "Topic: 5 \n",
            "Words: 0.099*\"one\" + 0.029*\"day\" + 0.028*\"man\" + 0.027*\"year\" + 0.025*\"two\" + 0.022*\"general\" + 0.017*\"back\" + 0.017*\"put\" + 0.017*\"church\" + 0.017*\"forc\"\n",
            "The first pre-processed document\n",
            "['assembl', 'session', 'bring', 'much', 'good']\n",
            "Checking the score as in to which topic this document belongs to\n",
            "\n",
            "Score: 0.4276925027370453\t \n",
            "Topic: 0.041*\"war\" + 0.039*\"like\" + 0.032*\"world\" + 0.032*\"mr\" + 0.032*\"time\" + 0.031*\"peac\" + 0.031*\"american\" + 0.028*\"know\" + 0.025*\"west\" + 0.025*\"editor\"\n",
            "\n",
            "Score: 0.2387199103832245\t \n",
            "Topic: 0.099*\"one\" + 0.029*\"day\" + 0.028*\"man\" + 0.027*\"year\" + 0.025*\"two\" + 0.022*\"general\" + 0.017*\"back\" + 0.017*\"put\" + 0.017*\"church\" + 0.017*\"forc\"\n",
            "\n",
            "Score: 0.2330029010772705\t \n",
            "Topic: 0.040*\"new\" + 0.039*\"come\" + 0.036*\"us\" + 0.028*\"leav\" + 0.026*\"test\" + 0.025*\"mr\" + 0.025*\"take\" + 0.022*\"presid\" + 0.021*\"see\" + 0.021*\"must\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_compiled= pd.DataFrame({'Index_Sent': pd.Series(dtype='int'),\n",
        "                             'Sentence': pd.Series(dtype='str'),\n",
        "                             'LDA_topic': pd.Series(dtype='int'),\n",
        "                            'LDA_score': pd.Series(dtype='float')})\n",
        "for i in range(len(bag_of_words_corpus)):\n",
        "    try:\n",
        "        data_compiled=data_compiled.append({'Index_Sent': i,\n",
        "                                        'Sentence':editorials[i],\n",
        "                                        'LDA_topic': \n",
        "                                        sorted(lda_final_model[bag_of_words_corpus[i]], key=lambda tup: -1*tup[1])[0][0],\n",
        "                                            'LDA_score': \n",
        "                                        sorted(lda_final_model[bag_of_words_corpus[i]], key=lambda tup: -1*tup[1])[0][1]\n",
        "                                           }, ignore_index=True)\n",
        "    except:\n",
        "        continue"
      ],
      "metadata": {
        "id": "t2u5FW4P6spk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "if compute_using_bag_of_words:\n",
        "    lda_vizualization = gensimvis.prepare(lda_final_model,bag_of_words_corpus,dictionary,sort_topics=False)\n",
        "    pyLDAvis.save_html(lda_vizualization,'LDA_visualization_bow.html')\n",
        "else:\n",
        "    lda_vizualization = gensimvis.prepare(lda_final_model,corpus_tf_idf,dictionary,sort_topics=False)\n",
        "    pyLDAvis.save_html(lda_vizualization,'LDA_visualization_tf_idf.html')"
      ],
      "metadata": {
        "id": "0u6ixTvUDzsW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "eVLXY7uZ6va0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_compiled[data_compiled['LDA_topic']==5].sort_values(by='LDA_score', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "qYAn5qEm6v-k",
        "outputId": "08b50787-a4fa-4da1-b237-1c95ba873703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Index_Sent  \\\n",
              "40            43   \n",
              "1464        1614   \n",
              "903          970   \n",
              "1120        1214   \n",
              "2087        2311   \n",
              "2206        2434   \n",
              "2493        2755   \n",
              "273          290   \n",
              "1677        1858   \n",
              "270          286   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                               Sentence  \\\n",
              "40    It includes a raise in the county minimum wage , creation of several new jobs at the executive level , financing of beefed-up industrial development efforts , and increased expenditures for essential services such as health and welfare , fire protection , sanitation and road maintenance .   \n",
              "1464                                                                                              These programs are volumes of waste paper and lost hours if the citizens of a community must stand aside while land developers tell them when , where , and in what manner the community shall grow .   \n",
              "903                                                                                                                                                                                This was just Richard's way of saying that last year the Birds opened spring training with a lot of jobs wide open .   \n",
              "1120                                                                                                                                          In case of a deadlock between prison boards and inmates , a federal arbitration board to include a `` lifer '' and two escapees should decide the issue .   \n",
              "2087                                                                                                                                                          Since appeals to morality , to humanity , and to sanity have had such small effect , perhaps our last recourse is the deterrent example .   \n",
              "2206                                                                                                    Our complaint is that in many crucial areas the Kennedy programs are not too large but too small , most seriously in regard to the conventional arms build-up and in aid and welfare measures .   \n",
              "2493                                                                                                                                                                                                                                                        The next days may show where things stand .   \n",
              "273                                                                                                                  The board of suspension of the Interstate Commerce commission has ordered a group of railroads not to reduce their freight rates on grain , as they had planned to do this month .   \n",
              "1677                                                                                                                                                It is a revelation of what has been done , what is being done and what will be done in Newark as shown by architects' plans , models and pictures .   \n",
              "270                                                                                  The same can be said about the half-hearted Cuban invasion mounted by the administration last April , which , we trust , is not symptomatic of the methods to be invoked in holding off the felonious Khrushchev .   \n",
              "\n",
              "      LDA_topic  LDA_score  \n",
              "40            5   0.923529  \n",
              "1464          5   0.915767  \n",
              "903           5   0.895292  \n",
              "1120          5   0.880744  \n",
              "2087          5   0.879961  \n",
              "2206          5   0.879365  \n",
              "2493          5   0.878852  \n",
              "273           5   0.860653  \n",
              "1677          5   0.860491  \n",
              "270           5   0.860486  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2df1dde2-a072-4b7b-a412-93fbb145f70a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index_Sent</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>LDA_topic</th>\n",
              "      <th>LDA_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>43</td>\n",
              "      <td>It includes a raise in the county minimum wage , creation of several new jobs at the executive level , financing of beefed-up industrial development efforts , and increased expenditures for essential services such as health and welfare , fire protection , sanitation and road maintenance .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.923529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>1614</td>\n",
              "      <td>These programs are volumes of waste paper and lost hours if the citizens of a community must stand aside while land developers tell them when , where , and in what manner the community shall grow .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.915767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>970</td>\n",
              "      <td>This was just Richard's way of saying that last year the Birds opened spring training with a lot of jobs wide open .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.895292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120</th>\n",
              "      <td>1214</td>\n",
              "      <td>In case of a deadlock between prison boards and inmates , a federal arbitration board to include a `` lifer '' and two escapees should decide the issue .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.880744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>2311</td>\n",
              "      <td>Since appeals to morality , to humanity , and to sanity have had such small effect , perhaps our last recourse is the deterrent example .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.879961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2206</th>\n",
              "      <td>2434</td>\n",
              "      <td>Our complaint is that in many crucial areas the Kennedy programs are not too large but too small , most seriously in regard to the conventional arms build-up and in aid and welfare measures .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.879365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>2755</td>\n",
              "      <td>The next days may show where things stand .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.878852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>290</td>\n",
              "      <td>The board of suspension of the Interstate Commerce commission has ordered a group of railroads not to reduce their freight rates on grain , as they had planned to do this month .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.860653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1677</th>\n",
              "      <td>1858</td>\n",
              "      <td>It is a revelation of what has been done , what is being done and what will be done in Newark as shown by architects' plans , models and pictures .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.860491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>286</td>\n",
              "      <td>The same can be said about the half-hearted Cuban invasion mounted by the administration last April , which , we trust , is not symptomatic of the methods to be invoked in holding off the felonious Khrushchev .</td>\n",
              "      <td>5</td>\n",
              "      <td>0.860486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2df1dde2-a072-4b7b-a412-93fbb145f70a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2df1dde2-a072-4b7b-a412-93fbb145f70a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2df1dde2-a072-4b7b-a412-93fbb145f70a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names= pd.DataFrame({'LDA_topic': pd.Series(dtype='int'),\n",
        "                             'topic_name': pd.Series(dtype='str'),\n",
        "                             })\n",
        "topic_names=topic_names.append({'LDA_topic': 0, 'topic_name':  \"Governments & Nations\" }, ignore_index=True)\n",
        "topic_names=topic_names.append({'LDA_topic': 1, 'topic_name':  \"People & Political Parties\" }, ignore_index=True)\n",
        "topic_names=topic_names.append({'LDA_topic': 2, 'topic_name':  \"War & Power\" }, ignore_index=True)\n",
        "topic_names=topic_names.append({'LDA_topic': 3, 'topic_name':  \"Presidents and Politics\" }, ignore_index=True)\n",
        "topic_names=topic_names.append({'LDA_topic': 4, 'topic_name':  \"General Debates of People\" }, ignore_index=True)\n",
        "topic_names=topic_names.append({'LDA_topic': 5, 'topic_name':  \"Developement adn Industry\" }, ignore_index=True)"
      ],
      "metadata": {
        "id": "o_WjYn44666Z"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_compiled=data_compiled.merge(topic_names, on='LDA_topic', how='left')"
      ],
      "metadata": {
        "id": "rv213Hid-ql6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_compiled.groupby('LDA_topic').head(5).to_csv(\"Sentences for Grounded_Coding.csv\")"
      ],
      "metadata": {
        "id": "AcqfgMwX_GDj"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}